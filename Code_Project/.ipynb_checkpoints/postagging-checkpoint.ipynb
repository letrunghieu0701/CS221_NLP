{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import WhitespaceTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_space_tokenizer = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_5_sentences = \"data.txt\"\n",
    "file_name_40_sentences = \"data_40_cau_tieng_Viet.txt\"\n",
    "\n",
    "file_name_dictionary = \"Viet74K.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of senteces in data: 2\n"
     ]
    }
   ],
   "source": [
    "sentences = list()\n",
    "\n",
    "with open(file_name_5_sentences, \"r\", encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    for i in range(len(content)):\n",
    "        sentence = content[i]\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "print(\"Number of senteces in data:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms in dictionary: 73901\n"
     ]
    }
   ],
   "source": [
    "dictionary_corpus = list()\n",
    "\n",
    "with open(file_name_dictionary, \"r\", encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    for i in range(len(content)):\n",
    "        term = content[i].strip().lower()\n",
    "        dictionary_corpus.append(term)\n",
    "        \n",
    "dictionary_corpus\n",
    "\n",
    "print(\"Number of terms in dictionary:\", len(dictionary_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessString(string_data):\n",
    "    # Lower the text\n",
    "    preprocess_data = string_data.lower()\n",
    "    \n",
    "    # Remove punctuations, each punctuation = space, ex: \"\"information @#$retrieval\" -> \"information    retrieval\"\n",
    "    preprocess_data = re.sub('[%s]' % re.escape(string.punctuation), ' ', preprocess_data)   \n",
    "        \n",
    "    # Tokenize word by white space\n",
    "    preprocess_data = white_space_tokenizer.tokenize(preprocess_data)\n",
    "      \n",
    "    return preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTerm(sentence_tokenized, start_index, end_index):\n",
    "    term = \"\"\n",
    "    for i in range(start_index, end_index):\n",
    "        term += sentence_tokenized[i] + \" \"\n",
    "    \n",
    "    return term.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_MAX_TERM_VIETNAMESE = 4\n",
    "\n",
    "sentences_tokenized = dict()\n",
    "\n",
    "for ith_sentence, sentence in enumerate(sentences):\n",
    "    \"\"\" Hiện tại thì chỗ này xét cả câu luôn, về sau sẽ nâng cấp lên thành xét các vế trong câu \"\"\"\n",
    "    sentence_part_tokenzied = PreprocessString(sentence)\n",
    "    \n",
    "    ith_start_term = 0\n",
    "    if len(sentence_part_tokenzied) > LEN_MAX_TERM_VIETNAMESE:\n",
    "        ith_end_term = ith_start_term + LEN_MAX_TERM_VIETNAMESE\n",
    "    else:\n",
    "        ith_end_term = len(sentence_part_tokenzied)\n",
    "        \n",
    "    term = GetTerm(sentence_part_tokenzied, ith_start_term, ith_end_term)\n",
    "    \n",
    "    # Xét hết cả vế\n",
    "    while True:\n",
    "        \n",
    "        # Xét hết cả term\n",
    "        while ith_start_term != ith_end_term:\n",
    "            if term in dictionary_corpus:\n",
    "                \n",
    "                if ith_sentence not in sentences_tokenized.keys():\n",
    "                    sentences_tokenized[ith_sentence] = [term]\n",
    "                else:\n",
    "                    sentences_tokenized[ith_sentence].append(term)\n",
    "#                 print(\"Trùng được term:\", term)\n",
    "                    \n",
    "                break\n",
    "            else:\n",
    "#                 print(\"không trùng term nào cả, term:\", term)\n",
    "                ith_end_term -= 1\n",
    "                term = GetTerm(sentence_part_tokenzied, ith_start_term, ith_end_term)\n",
    "\n",
    "        # Đã xét xong term cuối cùng thì chuyển sang câu (vế) khác\n",
    "        if ith_end_term == len(sentence_part_tokenzied):\n",
    "            break\n",
    "        # Nếu vẫn còn term để xét thì chuyển sang term kế tiếp\n",
    "        else:\n",
    "            # Đề phòng trường hợp term không có trong dictionary thì end sẽ tiến về và bằng start, và sẽ gây ra vòng lặp vô tận\n",
    "            # nếu cứ gắn start bằng lại end (giá trị start ban đầu) và cộng end = start + LEN_MAX_TERM_VIETNAMESE\n",
    "            # vd: term = \"2 mũi vaccine cơ\", start = 16, end = 20, chữ \"2\" không có trong dictionary nên sẽ khiến end bị trừ\n",
    "            # thành 16 (bằng với start), và nếu không có điều kiện kiểm tra \"ith_start_term == ith_end_term\" thì sẽ khiến\n",
    "            # start và end trở lại như cũ (16 và 20), vì ở đây gán start = end và đoạn bên dưới gán end = start + LEN_MAX_TERM_VIETNAMESE\n",
    "            if ith_start_term == ith_end_term:\n",
    "                ith_start_term = ith_end_term + 1\n",
    "            else:\n",
    "                ith_start_term = ith_end_term\n",
    "                \n",
    "            # Nếu vẫn còn nhiều hơn 4 term để xét thì chuyển sang 4 term kế tiếp\n",
    "            if len(sentence_part_tokenzied) - ith_end_term > LEN_MAX_TERM_VIETNAMESE:\n",
    "                ith_end_term = ith_start_term + LEN_MAX_TERM_VIETNAMESE\n",
    "            # Nếu chỉ còn nhiều nhất là 4 term thì lấy ra những term cuối cùng của vế để xét\n",
    "            else:\n",
    "                ith_end_term = len(sentence_part_tokenzied)\n",
    "                \n",
    "            term = GetTerm(sentence_part_tokenzied, ith_start_term, ith_end_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [e for e in sentences_tokenized.values() ]\n",
    "list_words = []\n",
    "list_pos = []\n",
    "for e in l:\n",
    "    pos = [i for i in e if len(i) ==1 ]\n",
    "    words = [i for i in e if len(i) > 1 ]\n",
    "    list_words.append(words)\n",
    "    list_pos.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tôi', 'chơi', 'cầu_lông'], ['tôi', 'đi', 'tập', 'thể_dục']]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list_words)):\n",
    "    for j in range(len(list_words[i])):\n",
    "        if ' ' in list_words[i][j]:\n",
    "            list_words[i][j] = list_words[i][j].replace(' ','_')\n",
    "\n",
    "print(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tôi', 'chơi', 'cầu_lông'], ['tôi', 'đi', 'tập', 'thể_dục']]\n",
      "[['n', 'v', 'a'], ['n', 'v', 'v', 'a']]\n"
     ]
    }
   ],
   "source": [
    "print(list_words)\n",
    "print(list_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tìm ra tất cả các pos có trong data  không trùng nhau \n",
    "def findPosAndWords(list_pos,list_words):\n",
    "    tmp1 = []\n",
    "    tmp2 = []\n",
    "    set_pos =set()\n",
    "    set_words = set()\n",
    "    # convert all to set \n",
    "    for i in list_pos:\n",
    "        tmp1.append(set(i))\n",
    "    for i in list_words:\n",
    "        tmp2.append(set(i))\n",
    "    # Union all \n",
    "    for i in tmp1:\n",
    "        set_pos = set.union (set_pos,i)\n",
    "    for i in tmp2:\n",
    "        set_words = set.union (set_words,i)\n",
    "    return list(set_words),list(set_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tôi', 'cầu_lông', 'thể_dục', 'tập', 'đi', 'chơi'] ['v', 'n', 'a']\n"
     ]
    }
   ],
   "source": [
    "set_words, set_pos = findPosAndWords(list_pos,list_words)\n",
    "print(set_words,set_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dicA(set_pos):\n",
    "    dicA = dict()\n",
    "    set_pos1  = ['s'] + set_pos \n",
    "    for i in set_pos1:\n",
    "        for j in set_pos:\n",
    "            key = i+j \n",
    "            dicA[key] = 0 \n",
    "    return dicA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sv': 0, 'sn': 0, 'sa': 0, 'vv': 0, 'vn': 0, 'va': 0, 'nv': 0, 'nn': 0, 'na': 0, 'av': 0, 'an': 0, 'aa': 0}\n"
     ]
    }
   ],
   "source": [
    "dicA = init_dicA(set_pos)\n",
    "print(dicA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dicB(set_words,set_pos):\n",
    "    dicB= dict()\n",
    "    for i in set_pos:\n",
    "        for j in set_words:\n",
    "            key = i+j \n",
    "            dicB[key] = 0 \n",
    "    return dicB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vtôi': 0, 'vcầu_lông': 0, 'vthể_dục': 0, 'vtập': 0, 'vđi': 0, 'vchơi': 0, 'ntôi': 0, 'ncầu_lông': 0, 'nthể_dục': 0, 'ntập': 0, 'nđi': 0, 'nchơi': 0, 'atôi': 0, 'acầu_lông': 0, 'athể_dục': 0, 'atập': 0, 'ađi': 0, 'achơi': 0}\n"
     ]
    }
   ],
   "source": [
    "dicB = init_dicB(set_words,set_pos)\n",
    "print(dicB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos_A(list_pos, dicA):\n",
    "    for i in range(0, len(list_pos)):\n",
    "        key = 's' + list_pos[i]\n",
    "        if i == 0: \n",
    "            dicA[key] +=1\n",
    "  \n",
    "    for i in range(0,len(list_pos)):\n",
    "        for j in range(i,len(list_pos)):\n",
    "            key = list_pos[i]+list_pos[j]\n",
    "            if j == i+1:\n",
    "                dicA[key] +=1\n",
    "\n",
    "    return dicA \n",
    "            \n",
    "\n",
    "# [['a', 'a', 'n', 'v', 'a', 'n', 'a'], ['a', 'a', 'n', 'v', 'v', 'a', 'n', 'n']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrixA(list_pos,dicA):\n",
    "    for e in list_pos:\n",
    "        count_pos_A(e,dicA )\n",
    "    return dicA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sv': 0, 'sn': 2, 'sa': 0, 'vv': 1, 'vn': 0, 'va': 2, 'nv': 2, 'nn': 0, 'na': 0, 'av': 0, 'an': 0, 'aa': 0}\n"
     ]
    }
   ],
   "source": [
    "A = matrixA (list_pos,dicA)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos_B(list_words,list_pos,dicB):\n",
    "    for (item1,item2) in zip(list_pos,list_words):\n",
    "        for (item3,item4 )in zip (item1,item2):\n",
    "            key = item3 + item4 \n",
    "            dicB[key]+=1 \n",
    "    return dicB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vtôi': 0, 'vcầu_lông': 0, 'vthể_dục': 0, 'vtập': 1, 'vđi': 1, 'vchơi': 1, 'ntôi': 2, 'ncầu_lông': 0, 'nthể_dục': 0, 'ntập': 0, 'nđi': 0, 'nchơi': 0, 'atôi': 0, 'acầu_lông': 1, 'athể_dục': 1, 'atập': 0, 'ađi': 0, 'achơi': 0}\n"
     ]
    }
   ],
   "source": [
    "B = count_pos_B(list_words,list_pos,dicB)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def smoothing(dic):\n",
    "    for e in dic.keys():\n",
    "        dic[e]=  dic[e] +1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sv': 1, 'sn': 3, 'sa': 1, 'vv': 2, 'vn': 1, 'va': 3, 'nv': 3, 'nn': 1, 'na': 1, 'av': 1, 'an': 1, 'aa': 1}\n",
      "{'vtôi': 1, 'vcầu_lông': 1, 'vthể_dục': 1, 'vtập': 2, 'vđi': 2, 'vchơi': 2, 'ntôi': 3, 'ncầu_lông': 1, 'nthể_dục': 1, 'ntập': 1, 'nđi': 1, 'nchơi': 1, 'atôi': 1, 'acầu_lông': 2, 'athể_dục': 2, 'atập': 1, 'ađi': 1, 'achơi': 1}\n"
     ]
    }
   ],
   "source": [
    "smoothing(A)\n",
    "print(A)\n",
    "smoothing(B)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSumA():\n",
    "    newdict = A.copy()\n",
    "    for key, value in A.items() :\n",
    "        key = key[0]+'sum'\n",
    "        if key in newdict: \n",
    "            newdict[key] += value\n",
    "        else:\n",
    "            newdict[key] = value\n",
    "            \n",
    "    return newdict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSumB():\n",
    "    newdict = B.copy()\n",
    "    for key, value in B.items() :\n",
    "        key = key[0]+'sum'\n",
    "        if key in newdict: \n",
    "            newdict[key] += value\n",
    "        else:\n",
    "            newdict[key] = value\n",
    "            \n",
    "    return newdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sv': 1, 'sn': 3, 'sa': 1, 'vv': 2, 'vn': 1, 'va': 3, 'nv': 3, 'nn': 1, 'na': 1, 'av': 1, 'an': 1, 'aa': 1, 'ssum': 5, 'vsum': 6, 'nsum': 5, 'asum': 3}\n"
     ]
    }
   ],
   "source": [
    "A  = calculateSumA()\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vtôi': 1, 'vcầu_lông': 1, 'vthể_dục': 1, 'vtập': 2, 'vđi': 2, 'vchơi': 2, 'ntôi': 3, 'ncầu_lông': 1, 'nthể_dục': 1, 'ntập': 1, 'nđi': 1, 'nchơi': 1, 'atôi': 1, 'acầu_lông': 2, 'athể_dục': 2, 'atập': 1, 'ađi': 1, 'achơi': 1, 'vsum': 9, 'nsum': 8, 'asum': 8}\n"
     ]
    }
   ],
   "source": [
    "B = calculateSumB()\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePro(dic):\n",
    "#     newdict = A.copy()\n",
    "    for key in dic:\n",
    "        key_sum = key[0]+'sum'\n",
    "        dic[key] =  dic[key]/dic[key_sum]\n",
    "    return dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sv': 0.2, 'sn': 0.6, 'sa': 0.2, 'vv': 0.3333333333333333, 'vn': 0.16666666666666666, 'va': 0.5, 'nv': 0.6, 'nn': 0.2, 'na': 0.2, 'av': 0.3333333333333333, 'an': 0.3333333333333333, 'aa': 0.3333333333333333, 'ssum': 1.0, 'vsum': 1.0, 'nsum': 1.0, 'asum': 1.0}\n"
     ]
    }
   ],
   "source": [
    "A = calculatePro(A)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vtôi': 0.1111111111111111, 'vcầu_lông': 0.1111111111111111, 'vthể_dục': 0.1111111111111111, 'vtập': 0.2222222222222222, 'vđi': 0.2222222222222222, 'vchơi': 0.2222222222222222, 'ntôi': 0.375, 'ncầu_lông': 0.125, 'nthể_dục': 0.125, 'ntập': 0.125, 'nđi': 0.125, 'nchơi': 0.125, 'atôi': 0.125, 'acầu_lông': 0.25, 'athể_dục': 0.25, 'atập': 0.125, 'ađi': 0.125, 'achơi': 0.125, 'vsum': 1.0, 'nsum': 1.0, 'asum': 1.0}\n"
     ]
    }
   ],
   "source": [
    "B = calculatePro(B)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMM = {\n",
    "    'A': A,\n",
    "    'B': B,\n",
    "    'words': set_words,\n",
    "    'pos': set_pos\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(sentence, hidden_states):\n",
    "#### Khởi tạo Viterbi #####################\n",
    "    T = len(sentence.split())\n",
    "    N = len(HMM['pos'])\n",
    "    \n",
    "    sent_tokens = sentence.split()\n",
    "    viterbi = np.zeros(shape=(N, T))\n",
    "    back_pointer = np.zeros(shape=(N, T), dtype=np.int8)\n",
    "    # Khởi tạo xong #######################\n",
    "    \n",
    "    \n",
    "#### Bắt đầu tính toán cho Viterbi #####################\n",
    "    # Tính toán cho từ đầu tiên trong câu\n",
    "    for s, pos in enumerate(HMM['pos']):\n",
    "        viterbi[s, 0] = HMM['A']['s'+pos] * HMM['B'][pos+sent_tokens[0]]\n",
    "        back_pointer[s, 0] = 0\n",
    "    \n",
    "    # Tính toán cho các từ tiếp theo trong câu\n",
    "    for t, word in enumerate(sent_tokens[1:]):\n",
    "        t = t+1\n",
    "        for s, pos in enumerate(HMM['pos']):\n",
    "            max_pos2pos2word = -999999\n",
    "            argmax = 0\n",
    "            for s_, pos_ in enumerate(HMM['pos']):\n",
    "                pos2pos2word = viterbi[s_][t-1] * HMM['A'][pos_+pos] * HMM['B'][pos+word]\n",
    "                if (pos2pos2word > max_pos2pos2word):\n",
    "                    max_pos2pos2word = pos2pos2word\n",
    "                    argmax = s_\n",
    "            viterbi[s, t] = max_pos2pos2word\n",
    "            back_pointer[s, t] = argmax\n",
    "    # Tính toán xong ####################################\n",
    "    \n",
    "    \n",
    "#### Bắt đầu gán nhãn cho từng từ trong câu\n",
    "    # Lấy nhãn cho từ cuối trong câu\n",
    "    best_pos_ith_of_last_word = np.argmax(viterbi[:, T-1])\n",
    "    pos_of_last_word = HMM['pos'][best_pos_ith_of_last_word]\n",
    "\n",
    "\n",
    "    # Lấy back pointer của từ cuối trong câu\n",
    "    back_pointer_of_next_word = back_pointer[best_pos_ith_of_last_word, T-1]\n",
    "    back_pointer_of_next_word\n",
    "    \n",
    "    print(best_pos_ith_of_last_word, pos_of_last_word)\n",
    "\n",
    "    for i in range(T-2, -1, -1):\n",
    "        # Lấy nhãn cho từ thứ i trong câu\n",
    "        best_pos_ith = back_pointer_of_next_word\n",
    "        pos = HMM['pos'][best_pos_ith]\n",
    "\n",
    "        # Lấy back pointer của từ thứ i trong câu\n",
    "        back_pointer_of_next_word = back_pointer[best_pos_ith, i]\n",
    "        back_pointer_of_next_word\n",
    "        print(best_pos_ith, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tôi', 'cầu_lông', 'thể_dục', 'tập', 'đi', 'chơi']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tôi', 'đi', 'tập', 'cầu_lông']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'tôi đi tập cầu_lông'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 a\n",
      "0 v\n",
      "0 v\n",
      "1 n\n"
     ]
    }
   ],
   "source": [
    "Viterbi('tôi đi tập cầu_lông', HMM['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
